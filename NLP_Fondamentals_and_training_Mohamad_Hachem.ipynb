{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "728dff35",
   "metadata": {},
   "source": [
    "# Welcome To The world Of NLP (Natural language processing)\n",
    "### What are we going to cover:\n",
    "<u>NLP Fundamentals:</u>\n",
    "<ul>\n",
    "    <li>Bag-of-Words</li>\n",
    "    <li>Word to Vectors</li>\n",
    "</ul>\n",
    "<u>Misc NLP Techniques:</u>\n",
    "<ul>\n",
    "    <li>Regexes</li>\n",
    "    <li>Stemming & Lemmatization</li>\n",
    "    <li>Spell Correction</li>\n",
    "    <li>Part of Speech Tagging</li>\n",
    "    <li>A few other techniques</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b7d2c4",
   "metadata": {},
   "source": [
    "### Before we start, What is NLP?\n",
    "<p>Natural language processing (NLP) refers to the branch of computer science—and more specifically, the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.</p>\n",
    "<h4> How do we make a computer understand?</h4>\n",
    "<p>we make the computer understand by applying a series of multiple approaches. Every approach is unique in it is own way.</p>\n",
    "<p> in the follwoing we will be visiting some of the approaches and I will be explaining how does it function.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933c079",
   "metadata": {},
   "source": [
    "#### 1-We will start with Bag of words:\n",
    "<u> What is the Bag of words approach?</u>\n",
    "<p>It is usually easier for a machine to understand numerical values such as 1 / 0 rather than \"dog\" \"house\"... The idea behind Bag of words is it to transform the words(Strings) into vectors.</p>\n",
    "<p>The bag of words approach tells us if any given string of a selected set is present in a text by simply putting one if it does and 0 if it doesn't.</p>\n",
    "<u>For example:</u>\n",
    "<p>Let's say we choose the following set: {'dog','love','play'}. now we want to see if These words exists in the following sentenses:</p>\n",
    "<ul>\n",
    "    <li>I have a dog</li>\n",
    "    <li>I love apples</li>\n",
    "    <li>I am playing video games</li>\n",
    "</ul>\n",
    "<p>with the bag of words approah we would have the following matrix result</p>\n",
    "[[1,0,0],<br>\n",
    " [0,1,0],<br>\n",
    " [0,0,1]]<br>\n",
    "<p> such results are easier to work with on the computer scale rather than words. Let's move to a more practical scale example.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057792c0",
   "metadata": {},
   "source": [
    "We will use SKlearn module to for the following work since it contains the needed tools we need.<br>\n",
    "for more info/documentation check the following link:\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e081ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importimg the needed module\n",
    "from sklearn.feature_extraction.text import CountVectorizer # cound be a 1 or 0 to see if a certain word exists or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab73863",
   "metadata": {},
   "source": [
    "Let's Start with creating the sets we are going to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bad75f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create categories to tell our training set train_X to which category every item belongs too\n",
    "class Category:\n",
    "    BOOKS = \"BOOKS\"\n",
    "    CLOTHING = \"CLOTHING\"\n",
    "    \n",
    "# creating a train list train_x(sentences/items) and train_y(Categories every item belongs to)\n",
    "train_x = [\"I love the book\", \"This is a great book\",\"this fit is great\",\"this love the shoes\"]\n",
    "train_y = [Category.BOOKS, Category.BOOKS, Category.CLOTHING, Category.CLOTHING]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80440a9f",
   "metadata": {},
   "source": [
    "Now let's start applying bags of words approach on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffb8caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to use the vectorizer to transform the training set to a matrix\n",
    "# CountVectorizer Converts a collection of text documents to a matrix of token counts.\n",
    "vectorizer = CountVectorizer(binary=True) # side note we can also work with more than one word(for positive/negative for example) using ngram_range(1,2) but for the sake of simplicity we will continue like this\n",
    "\n",
    "# we are going to use fit_transform on the train_X to covert it to a matrix \n",
    "vectors = vectorizer.fit_transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8278ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['book', 'fit', 'great', 'is', 'love', 'shoes', 'the', 'this'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's visualise the list that we obtain (notice that every word is repeated only once from my training set)\n",
    "# it also get rid of words that it sees is not important (such as \"I\")\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50132018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the see the matrix that we obtains from the vectors\n",
    "# notice that the matrix simply tells us about the state of every element in the vectorizer in a specific sentence/element.\n",
    "# giving us 0 for no and 1 for yes such as \"book\" / \"Love\" / \"The\" in the first row of the matrix according\n",
    "# to the first sentence in tain_X\n",
    "vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe2a3a",
   "metadata": {},
   "source": [
    "Now that we have our Matrix it is easier for the machine to use we can therefore teach build a machine learning alogrithm.<br>\n",
    "for this one we are going to use SVM.<br>\n",
    "<u>What is SVM?</u>\n",
    "<p>SVM or Support Vector Machine is a linear model for classification and regression problems</p>\n",
    "<u>How Does it Work?</u>\n",
    "<p>SVM works by mapping data to a high-dimensional feature space so that data points can be categorized, even when the data are not otherwise linearly separable.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4b74fd",
   "metadata": {},
   "source": [
    "Let's create the model<br>\n",
    "for documentation/info check this link:https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3f44b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the Needed Module\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7706a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to use SVC since it is good for small amount of Data\n",
    "clf_svm = svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40c24ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's fit our model\n",
    "clf_svm.fit(vectors, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa79a3",
   "metadata": {},
   "source": [
    "Let's create the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54f1f24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating our testing set\n",
    "test_X = vectorizer.transform(['i like the book','shoes are alright'])\n",
    "\n",
    "# let's visualize\n",
    "test_X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daacc9b",
   "metadata": {},
   "source": [
    "Let's test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b86b50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BOOKS', 'CLOTHING'], dtype='<U8')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting our model\n",
    "clf_svm.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c685d",
   "metadata": {},
   "source": [
    "as we can see our model predicted the topic to be book related then clothing related. Which is what we wanted.<br>\n",
    "<b>note:</b> some downside of bag of words is that the model won't know what to do if the word didn't occure before such as books since for the machine \"books\" is name the same as \"book\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c043f57d",
   "metadata": {},
   "source": [
    "#### 2- word vectors\n",
    "<u> What is the Word Vectors approach?</u>\n",
    "<p>Word embedding or word vector is an approach with which we represent documents and words. It is defined as a numeric vector input that allows words with similar meanings to have the same representation. It can approximate meaning and represent a word in a lower dimensional space.</p>\n",
    "<p><b>In simpler words</b> word vectors approach is the idea of making the machine understand that certain words falls under the same umbrella in meaning.</p>\n",
    "<u>For example:</u>\n",
    "<p> let's take the following text \"This story has intersting characters\". with word vectors approach the machine will be able to understand that \"story\" and \"characters\" are close and fall under the umbrella of <i>Story</i></p>\n",
    "Let's move to a practical example to understand how does it work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b36893",
   "metadata": {},
   "source": [
    "For this approach I am going to use the <b> Spacy </b> Library.<br>\n",
    "Check this link for more info: https://spacy.io/usage/embeddings-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cfa755",
   "metadata": {},
   "source": [
    "For this we are going to download some pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d797a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the needed libraries / models I will be commenting this so it doesn't cause issues if someone want to run the whole thing\n",
    "#!pip install spacy\n",
    "# this is a medium sized model you can also use the large one for better results using en_core_web_lg\n",
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d2e8a",
   "metadata": {},
   "source": [
    "For more info/documentation for spacy check this link:https://spacy.io/api/vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7802e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the needed module\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc3ea024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the medium model that we downloaded\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cad5382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the normal representation : I love the book\n",
      "this is the vector representation : \n",
      " [-1.3978975  -0.94314    -1.1927751  -4.3043246  -1.69008    -2.18875\n",
      "  2.8568673   4.1011753  -3.6249747   4.17611     7.12575     2.2885249\n",
      " -6.495055   -0.703155    3.46178    -0.9427      4.1357403  -3.3983903\n",
      " -0.04899997  1.3973      1.5476775   1.4060001  -0.07001507 -4.591998\n",
      " -1.038875   -1.8461976  -3.6312752   0.4407499  -1.7652375   3.388475\n",
      " -0.4016     -1.471375   -0.39702505  0.24449998 -0.04592732 -1.4172026\n",
      " -1.167125    0.5882125   2.6957998  -0.5626705  -1.7447001   3.973075\n",
      " -0.671685   -1.0611899   4.576425    2.9842675  -2.49175    -2.6355624\n",
      "  0.5972425   0.59040004 -0.792125   -0.590725    0.33869502 -3.42171\n",
      " -3.4163604  -0.1711675  -0.786485    1.4665233   3.89455     1.9638373\n",
      "  5.5787754  -1.3022224  -0.651945    0.43172497 -2.4435027   0.596875\n",
      " -3.6072845  -5.0790253   3.3520503   3.8547673  -0.87257504  2.2705574\n",
      " -0.5900501  -2.054635    3.19281     3.36905    -2.8925076   1.6652\n",
      " -2.5049374  -2.7379746  -2.37408     0.8923325   5.0625834  -1.2852752\n",
      "  1.9027183   1.0960624  -0.52304995 -0.95255005 -1.9461374  -2.310925\n",
      " -1.098875    1.8177226   4.2221003  -6.537125    0.48603004 -3.50084\n",
      "  0.5391325  -0.50697494  1.2737525   0.521275    1.321665    3.1960826\n",
      "  2.002865    2.8237524  -1.3757352  -0.17954004 -1.6516751  -0.5081499\n",
      " -1.5433998  -1.4201249   1.3113251  -2.2571251   0.84857243  1.714025\n",
      "  1.82535     0.29791498 -2.233875   -2.3855677  -1.1692325   1.18858\n",
      " -2.2354374  -2.14905    -0.78332496  4.1412625   1.0825651   1.1799951\n",
      "  2.902325   -3.5272      0.34348005 -2.30878    -4.9016647   2.4042501\n",
      " -0.46282256 -2.743905   -2.4401073   3.803725   -3.1931999  -2.5288126\n",
      "  5.5490255  -5.0140996  -3.4048173   0.31995994  1.6301392   2.1406627\n",
      " -2.0564826   1.1080999  -1.593325    0.20552728 -3.6550498  -0.68221736\n",
      " -2.064635    3.63225    -0.97382474 -2.821975   -1.6865926   2.7102222\n",
      "  3.1241271   0.83275247 -1.4126899  -0.287584   -0.3687275  -5.2711525\n",
      " -1.8511002   1.1304475  -2.5447674   0.17523009 -2.3348875  -1.7706876\n",
      " -2.5483801  -0.93720007  3.9849     -1.0662625  -1.1497501   2.5656776\n",
      "  4.118925   -3.53025    -1.0182877   1.2404      1.2558925  -1.8724298\n",
      "  0.28065002 -0.24649996  5.1748123  -0.51769006  0.14942479  3.3902276\n",
      " -1.0006975  -3.0822291   4.4143      2.60167    -1.5609701   2.0564725\n",
      " -2.053175    1.6127999   2.1787999  -1.49475    -6.6126003   2.3603249\n",
      " -2.24825     6.54807    -1.2758749  -1.4058051  -4.0160775  -1.2164625\n",
      "  0.6155751  -2.1410775   1.1037402  -0.45322743 -0.76562977 -2.0316248\n",
      "  3.3534575   0.906615    1.78155     3.0272427  -1.490925    2.8774002\n",
      " -3.5095177   0.9973675  -0.25279248 -0.6435975  -1.468935    2.167055\n",
      " -1.5119425   5.634525   -1.8386     -0.34084988  0.37256616  3.5403275\n",
      "  2.133315    1.0783851  -3.0888252  -5.5216255   1.7827001  -1.00615\n",
      " -0.92117244  2.1876426  -3.1331475   6.675755   -1.2426828  -1.1250799\n",
      " -4.355845   -2.8189998   3.16365     1.7629576   5.450925   -0.41921014\n",
      " -2.0778     -2.4137576   4.885243    1.3854975  -1.960375   -0.20622507\n",
      " -6.011125   -1.2472365  -1.4729     -1.9700327  -0.14459997  0.09206754\n",
      " -4.2257      2.7291002   3.19189     3.539985    5.323       2.4565349\n",
      "  3.4231699  -3.2712998  -0.7532724   2.5230036  -6.0874753   1.398425\n",
      " -0.3443627  -3.8744974  -4.01758    -0.67568743 -1.0210674   0.18305254\n",
      "  0.52802503 -5.6001253  -0.54296505  2.2708325  -4.5382824  -2.6007726\n",
      "  2.742875    1.1766725   2.9447925  -1.3840849   2.25675     1.28454\n",
      " -1.8143474   0.09939742  3.9096823  -0.40517497  2.289015   -0.37639502\n",
      " -2.642429   -0.8754725   2.2959075  -0.20367491 -7.441825    1.10268   ]\n",
      "this is the shape of our vector : (300,)\n"
     ]
    }
   ],
   "source": [
    "# let's take the same training set we had before and apply this approach on it\n",
    "train_x =  [\"I love the book\", \"This is a great book\",\"this fit is great\",\"this love the shoes\"]\n",
    "\n",
    "# let's see what we will get when we apply our model on 1 of our sentences\n",
    "doc = nlp(train_x[0])\n",
    "print(f\"this is the normal representation : {doc}\")\n",
    "print(f\"this is the vector representation : \\n {doc.vector}\")\n",
    "print(f\"this is the shape of our vector : {doc.vector.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636105fb",
   "metadata": {},
   "source": [
    "The vector representation shows us the avg word embedding for our text \"I love the book\" showing us how close it is to other words using numerical representation. Now that we saw how this are for one text let's apply it for all the elements of our train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "268f0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying our model on all our items\n",
    "docs = [nlp(item) for item in train_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ef9da108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assign our vectors to seperate variables just for simplicity\n",
    "train_x_word_vectors = [doc.vector for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f7a673",
   "metadata": {},
   "source": [
    "Let's build another SVM model to test what we obtained like we did previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a0cc975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's build our model for word vectors (wv)\n",
    "clf_svm_wv = svm.SVC(kernel='linear')\n",
    "\n",
    "# Now let's fit our model\n",
    "clf_svm_wv.fit(train_x_word_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a41fe848",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BOOKS'], dtype='<U8')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's create our testing set\n",
    "test_x = [\"i love the book\"]\n",
    "test_docs = [nlp(text) for text in test_x]\n",
    "test_x_word_vectors = [doc.vector for doc in test_docs]\n",
    "\n",
    "# let's try to predict\n",
    "clf_svm_wv.predict(test_x_word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9008566",
   "metadata": {},
   "source": [
    "Notice that we got \"BOOK\" as classification which is normal because we had it in our training set. But what would happen if we tried using words that we know is close in meaning such as <b>\"story\"</b> rather than <b>\"book\"</b>. Let's test That!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f5505d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CLOTHING'], dtype='<U8')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's create our testing set\n",
    "test_x = [\"i love the story\"]\n",
    "test_docs = [nlp(text) for text in test_x]\n",
    "test_x_word_vectors = [doc.vector for doc in test_docs]\n",
    "\n",
    "# let's try to predict\n",
    "clf_svm_wv.predict(test_x_word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb07b856",
   "metadata": {},
   "source": [
    "Notice the power of word vectors approach!<br> unlike the bag of words where it couldn't predict things correctly(once we went out of the context of the used words in the training set), the word vectors approach was able to understand that story is close to book and therefore classify our test as BOOKS related.<br>\n",
    "\n",
    "<b>PS:</b> It can get sometimes tricky with word vectors specially when we are using the same word to mean different things because the vector representation for the same word is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec9b93",
   "metadata": {},
   "source": [
    "## Next NLP Techniques\n",
    "#### We are going to start with Regexes\n",
    "<u> What is Regexes?</u>\n",
    "<p>Regexes are pattern matching of strings in python such as:</p>\n",
    "<ul>\n",
    "    <li>Password checkers</li>\n",
    "    <li>Phone numbers</li>\n",
    "    <li>emails</li>\n",
    "</ul>\n",
    "<u>For example:</u>\n",
    "<p>Let's take these 2 phone numbers 1234567899 and +331234567899 both of them are valid numbers, but they look different and therefore regex would allow us to spot certain patterns to use both phone numbers.</p>\n",
    "<p> Let's move to a more practical scale example.</p>\n",
    "<p> for more info check the following link:<a>https://cheatography.com/davechild/cheat-sheets/regular-expressions/</a></p>\n",
    "<a>https://regex101.com/</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a96f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import the needed library\n",
    "import re\n",
    "\n",
    "# creating my regex\n",
    "# ^ in the beginning tell us that ab is the beginning of a string,\n",
    "# [^\\s] tell us that we allow everything except a white space between ab and cd\n",
    "# * tells us we can repeat [^\\s] at least 0 times \n",
    "regexp = re.compile(r\"^ab[^\\s]*cd$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fbc893b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcd', 'abxxxcd']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see some examples\n",
    "phrases = ['abcd','xxx','abxxxcd','ab cd']\n",
    "\n",
    "# only the first and third should match\n",
    "matches = []\n",
    "for phrase in phrases:\n",
    "    if re.match(regexp, phrase):\n",
    "        matches.append(phrase)\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67581186",
   "metadata": {},
   "source": [
    "As we can see the matches are exactly what we wanted it to be when we created our regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a0e87",
   "metadata": {},
   "source": [
    "### Next: Stemming/Lemmatization\n",
    "<u> What is Stemming/Lemmatization?</u>\n",
    "<p>Do you remember in the bag of words approach where book and books where seen differently by the machine the stemming/Lemmatization transform similar words to the origin of the word allowing the machine to understand that books and book are the same thing.</p>\n",
    "<p><b>Note:</b>Stemming and lemmatization are methods used by search engines and chatbots to analyze the meaning behind a word. Stemming uses the stem of the word, while lemmatization uses the context in which the word is being used.</p>\n",
    "<u>For example:</u>\n",
    "<ul>\n",
    "    <li>books -> book</li>\n",
    "    <li>reading -> read</li>\n",
    "    <li>stories -> stori/story</li>\n",
    "</ul>\n",
    "<p> Let's move to a more practical scale example.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8680e52a",
   "metadata": {},
   "source": [
    "for this technique we are going to work with NLTK module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67cc221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2022.9.13-cp310-cp310-win_amd64.whl (267 kB)\n",
      "Requirement already satisfied: click in c:\\users\\mhmd\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\mhmd\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mhmd\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mhmd\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.5)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.7 regex-2022.9.13\n"
     ]
    }
   ],
   "source": [
    "# downloading the package\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a3e2b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mhmd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mhmd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mhmd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the needed modules\n",
    "import nltk\n",
    "\n",
    "# we are going to download some important lists from the module to help us\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b1321c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# initalising our stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5ff5f9",
   "metadata": {},
   "source": [
    "For the stemmer to work it basically expects single word each time to search for its original word so we can't give it a full sentence what we need to do is take a sentence and transform it into words then give it to our stemmer to do the work we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "78200023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reading', 'the', 'stories']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating our example / test phrase\n",
    "phrase = \"reading the stories\"\n",
    "# splitting our phrase into words\n",
    "words = word_tokenize(phrase)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "edc0547f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read the stori'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see our stemmed words\n",
    "stemmed_words = [] \n",
    "for word in words:\n",
    "    stemmed_words.append(stemmer.stem(word))\n",
    "# joing the words into one sentence\n",
    "new_phrase = \" \".join(stemmed_words)\n",
    "new_phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cc1a7e",
   "metadata": {},
   "source": [
    "Now that we did the stemming let's try our Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ef11bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing our needed modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# initalising our lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d4d2141e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reading', 'the', 'stories']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are going to use the new_phrase as input\n",
    "new_phrase = phrase\n",
    "# splitting our phrase into words\n",
    "words = word_tokenize(new_phrase)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e21fdf5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reading the story'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see our lemmatized words\n",
    "lemmatized_words = [] \n",
    "for word in words:\n",
    "    lemmatized_words.append(lemmatizer.lemmatize(word))\n",
    "# joing the words into one sentence\n",
    "new_phrase = \" \".join(lemmatized_words)\n",
    "new_phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552c894",
   "metadata": {},
   "source": [
    "We can notice the difference between stemming and lemmatization.<br>\n",
    "<ul>\n",
    "    <li>'reading the story'</li>\n",
    "    <li>'read the stori'</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d4ce0",
   "metadata": {},
   "source": [
    "## Next Stopwords Removal\n",
    "<p>Stop words are basically the set of english words that are mostly common and we would like to remove them of our sentences because they don't add much meaning to our sentenses</p>\n",
    "<u>For example:</u>\n",
    "<ul>\n",
    "    <li>this</li>\n",
    "    <li>that</li>\n",
    "    <li>it</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7e45ac12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# importing our needed modules\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "# we notice that we have around 179 stop words\n",
    "print(len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "319bd65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'demonstrating',\n",
       " 'the',\n",
       " 'removal',\n",
       " 'of',\n",
       " 'stopwords']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's create an example\n",
    "test = \"Here is an example demonstrating the removal of stopwords\"\n",
    "\n",
    "# tokenizing our sentense\n",
    "words = word_tokenize(test)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b4a8e738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here', 'example', 'demonstrating', 'removal', 'stopwords']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's remove the stop words\n",
    "stripped_phrase = []\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        stripped_phrase.append(word)\n",
    "stripped_phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038be894",
   "metadata": {},
   "source": [
    "#### How is removing stop words helpful?\n",
    "removing stop words can really be helpful specially when we are working for example on our word vectors. By removing these words our calculation becomes more accurate when it comes to the meaning of our sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf929160",
   "metadata": {},
   "source": [
    "## Next: Spell correction/Tagging/Other techniques\n",
    "<p>As the name suggests Spell correction basically means taking a text and writing it without errors</p>\n",
    "<h4>How can this be benificial?</h4>\n",
    "<p>if for some reason we are collecting data let's say from twitter for example to process the data we need to take into consideration that people might mispelling words and use spell correction to work on that</p>\n",
    "check this link for reference:<a>https://textblob.readthedocs.io/en/dev/api_reference.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1250f4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "     -------------------------------------- 636.8/636.8 kB 9.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\mhmd\\appdata\\roaming\\python\\python310\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\mhmd\\appdata\\roaming\\python\\python310\\site-packages (from nltk>=3.1->textblob) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mhmd\\appdata\\roaming\\python\\python310\\site-packages (from nltk>=3.1->textblob) (2022.9.13)\n",
      "Requirement already satisfied: joblib in c:\\users\\mhmd\\appdata\\roaming\\python\\python310\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mhmd\\appdata\\roaming\\python\\python310\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mhmd\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk>=3.1->textblob) (0.4.5)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "# installing the textblob module\n",
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55475ecc",
   "metadata": {},
   "source": [
    "For this we are going to use TextBlob module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3347f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the needed modules\n",
    "from textblob import TextBlob\n",
    "#!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bebead7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the corrected form: 'this is not a good example'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('not', 'RB'),\n",
       " ('a', 'DT'),\n",
       " ('good', 'JJ'),\n",
       " ('examplee', 'NN')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating our example\n",
    "phrase = \"this is not a good examplee\"\n",
    "\n",
    "# let's convert our text to a textBlob Object\n",
    "tb_phrase = TextBlob(phrase)\n",
    "\n",
    "# the corrected sentense\n",
    "print(f\"the corrected form: '{tb_phrase.correct()}'\")\n",
    "\n",
    "# let's try to put tags on the words (noun/verb..?)\n",
    "tb_phrase.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d4288d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.35, subjectivity=0.6000000000000001, assessments=[(['not', 'good'], -0.35, 0.6000000000000001, None)])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some intresting things we can do with this library is the following\n",
    "tb_phrase.sentiment_assessments\n",
    "# we can basically see if it is positive or negative effect on our text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529d5483",
   "metadata": {},
   "source": [
    "For more intresting info check out the following link: <a>explosion.ai/blog/spacy-transformers</a> <br>\n",
    "for a deeper explenation check out and where I learned most of the things: https://www.youtube.com/watch?v=M7SWr5xObkA&t=1510s&ab_channel=KeithGalli"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
